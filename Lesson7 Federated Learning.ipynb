{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#******LESSON 7 IN spai CHALLENGE******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICE CODES ON SYFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  4.,  6.,  8., 10.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "x = th.Tensor([1,2,3,4,5])\n",
    "x\n",
    "y = x + x\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(th)\n",
    "th.tensor([1,2,3,4,5]) #DEMO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pretend machine and a pretend worker for nw\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "bob._objects #workers are sets of objects..output is {}, cuz bob doesnt have any stored or received value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=th.tensor([1,2,3,4,5])\n",
    "x=x.send(bob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob._objects #gvs output similar to {63827205014: tensor([1,2,3,4,5])}\n",
    "#it is proof that original tensor was sent to bob. we will be returned with a pointer to the remote object. print x nd check\n",
    "\n",
    "x \n",
    "#output is of form (Wrapper) > [PointerTensor | me:93174343354 -> bob:63827205014].x is a tensor. however instead of local\n",
    "#execution, each cmd is serialized to a simple JSON or tuple format sent to bob, he executes it on our behalf and send back the \n",
    "#pointer to the new object.\n",
    "\n",
    "x.location # can also do == bob as test condition or so\n",
    "#output similar to <VirtualWorker id:bob #tensors:1>\n",
    "\n",
    "x.id_at_location #63827205014\n",
    "x.id #93174343354\n",
    "x.owner #<VirtualWorker id:me #tensors:0>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.get()\n",
    "x #tensor([1,2,3,4,5])\n",
    "bob._objects #{} will be the output...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding another VirtualWorker\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "alice._objects #output is {} as of nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=th.tensor([1,2,3,4,5])\n",
    "x = x.send(bob, alice)\n",
    "#we can also use x=th.tensor([1,2,3,4,5]).send(bob, alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x #here x is a multipointerTensor with each sender having a PointerTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.child.child # returns a dictionary of sub PointerTensors of the MultiPointer\n",
    "x.get() # returns 2 copies of the tensor sent cuz we hv 2 workers\n",
    "\n",
    "x.get(sum_results = True) # returns a tensor ([2,4,6,8,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***INTRODUCING REMOTE ARITHMETIC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=th.tensor([1,2,3,4,5]).send(bob)\n",
    "y=th.tensor([1,1,1,1,1]).send(bob)\n",
    "\n",
    "x #diffrnt me,bob value from y. \n",
    "\n",
    "y\n",
    "\n",
    "z=x+y #returns me nd bob values equal to each other\n",
    "z\n",
    "\n",
    "z=z.get()\n",
    "z #returns ([2,3,4,5,6]) i.e. addition of the two msg tensors\n",
    "\n",
    "#we can also use z=th.add(x,y) z z=z.get() z to get the same output as abv...this is using torch funcn ashte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=th.tensor([1,2,3,4,5], requires_grad=True).send(bob)\n",
    "y=th.tensor([1,1,1,1,1], requires_grad=True).send(bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=(x+y).sum()\n",
    "z.backward() #back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.get()\n",
    "x #prints [1., 2., 3., 4., 5.]\n",
    "\n",
    "x.grad #prints [1., 1., 1., 1., 1.,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***LEARN A SIMPLE LINEAR MODEL WHICH IS ON SOME OTHER DEVICE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = th.tensor([[1.,1], [1,0], [0,1], [0,0]], requires_grad=True).send(bob)\n",
    "target = th.tensor([[1.], [1], [0], [0]]).send(bob)\n",
    "\n",
    "input #PointerTensor output\n",
    "target #PointerTensor output\n",
    "\n",
    "weights = th.tensor([[0.], [0.]], requires_grad=True).send(bob)\n",
    "\n",
    "for i in range(10):\n",
    "    preds = input.nn(weights)\n",
    "    loss = ((pred-target)**2).sum() #sum of mean squared error\n",
    "    loss.backward()\n",
    "\n",
    "    weights.data.sub_(weights.grad * 0.1)\n",
    "    weights.grad *= 0 #multiplying by zero so that they dont get accumulated over time\n",
    "\n",
    "    print(loss.get().data) #prints tensor(2.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***GARBAGE COLLECTION AND COMMON ERRORS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = bob.clear_objects()\n",
    "bob._objects #output is {}\n",
    "\n",
    "x=th.tensor([1,2,3,4,5]).send(bob)\n",
    "bob._objects #will contain the tensor sent to bob\n",
    "\n",
    "del x\n",
    "bob._objects #output is {}\n",
    "\n",
    "#this is controlled by a special attribute i.e. x.child.garbage_collect_data which is set to True by default.what this attribute\n",
    "#does is it sends a message to the Virtualworker, here, Bob to delete the tensor which was sent by us cuz we have deleted the \n",
    "#\"me\" pointer.\n",
    "\n",
    "#GOTCHA IN JUPYTER NOTEBOOK...this happens with reference to garbage collection of some pointers\n",
    "x=th.tensor([1,2,3,4,5]).send(bob)\n",
    "bob._objects #prints the output\n",
    "x=\"asdf\"\n",
    "bob._objects #output is {}\n",
    "\n",
    "#DOESNT HAPPEN EVERYTIME\n",
    "x=th.tensor([1,2,3,4,5]).send(bob)\n",
    "bob._objects #prints output\n",
    "x=\"asdf\"\n",
    "bob._objects #prints output\n",
    "del x\n",
    "bob._objects #prints output\n",
    "\n",
    "bob=bob.clear_objects()\n",
    "bob._objects #output is {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***TOY FEDERATED LEARNING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "#Create a toy dataset\n",
    "input = th.tensor([[1.,1], [1,0], [0,1], [0,0]], requires_grad=True)\n",
    "target = th.tensor([[1.], [1], [0], [0]])\n",
    "\n",
    "#Create toy model\n",
    "model=nn.Linear(2,1)\n",
    "\n",
    "opt=optim.SGD(param=model.parameters(), lr=0.1) #stochastic gradient descent\n",
    "\n",
    "def train(iterations=20):\n",
    "    for iter in range(iterations):\n",
    "        opt.zero_grad()\n",
    "\n",
    "        pred=model(data)\n",
    "\n",
    "        loss = ((pred-target)**2).sum()\n",
    "        #loss #output is tensor(0.4472, gradfunction=<SumBackward0>)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print(loss.data) #output is 0.4472"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing federated learning for above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bob = data[0:2].send(bob)\n",
    "target_bob = target[0:2].send(bob)\n",
    "\n",
    "data_alice = data[0:2].send(alice)\n",
    "target_alice = target[0:2].send(alice)\n",
    "\n",
    "datasets = [(data_bob, target_bob), (data_alice, target_alice)] #tuples in list\n",
    "\n",
    "def trains(iterations=20):\n",
    "    model = nn.Linear(2,1)\n",
    "    opt=optim.SGD(param=model.parameters(), lr=0.1)\n",
    "\n",
    "    for iter in range(iterations):\n",
    "        for _data, _target in datasets:\n",
    "            #_data #it is a PointerTensor from me to Bob\n",
    "            #_data.location #output is VirtualWorker with id Bob\n",
    "\n",
    "            \"\"\"Send model to remote device\"\"\"\n",
    "            #list(model.parameters()) #output is a list of weight nd bias values\n",
    "            model=model.send(_data.location)\n",
    "            #list(model.parameters()) #output is a list of pointers to data present elsewhere i.e. Bob's machine\n",
    "\n",
    "            \"\"\"Do normal training\"\"\"\n",
    "            opt.zero_grad()\n",
    "            pred=model(_data)\n",
    "            loss = ((pred-_target)**2).sum()\n",
    "            #loss #output is tensor(0.4472, gradfunction=<SumBackward0>)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            \"\"\"Get smarter model back (to the cloud maybe!)\"\"\"\n",
    "            model=model.get()\n",
    "            print(loss.get())\n",
    "\n",
    "#when bob's device trains only for 1 iteration            \n",
    "\"\"\"Difference between model sent and model got back can be used in reverse engineering to try and deduce data on the device\"\"\"\n",
    "#1 way to overcome : Train for many iterations on bob's device....\n",
    "#2 way to overcome : instead of bringing model back to us directly and then sending to alice, train the model parallely on many\n",
    "#                   workers so tht many changes are made and then we cant reverse engineer as to wht is the data nd whose is it?\n",
    "#                   we get back the average of multiple people's models!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADVANCED REMOTE EXECUTION, TRAINING MODEL PARALLELY ON MULTIPLE REMOTE DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clearing previously stored objects\n",
    "bob.clear_objects()\n",
    "alice.clear_objects()\n",
    "\n",
    "x=th.tensor([1,2,3,4,5]).send(bob) #x is wrapper to pointertensor\n",
    "bob._objects #tensor([1,2,3,4,5]) with a pointer id\n",
    "\n",
    "x = x.send(alice) #x will point to alice now\n",
    "alice._objects #wrapper to pointertensor but from bob to alice i.e. alice -> bob\n",
    "\n",
    "y = x + x #when we need to access the tensor[1,2,3,4,5], we use x which points to alice which uses a pointer to bob to access it\n",
    "y #output is a wrapper to pointertensor from me to alice \n",
    "\n",
    "bob._objects #contains 2 tensors [1,2,3,4,5] and [2,4,6,8,10]\n",
    "alice._objects #contains 2 pointers to the 2 tensors with bob\n",
    "\n",
    "#Bob and Alice are owners of the data. If either one of them doesnt allow us to modify data, for any reason, then we cannot \n",
    "#access it at all. The data has a dependency chain...from me to alice to bob. if alice forwards it to bob nd bob allows, then ok\n",
    "#if not or if alice doesnt forward at all, den not possible to access data.\n",
    "\n",
    "#create a 3rd worker and create 2 different dependency chains\n",
    "Jon=sy.VirtualWorker(hook, id=\"Jon\")\n",
    "\n",
    "bob.clear_objects()\n",
    "alice.clear_objects()\n",
    "\n",
    "x=th.tensor([1,2,3,4,5]).send(bob).send(alice)\n",
    "y=th.tensor([1,2,3,4,5]).send(bob).send(jon)\n",
    "\n",
    "bob._objects #will have two tensors [1,2,3,4,5] and [1,2,3,4,5]\n",
    "alice._objects #will have a pointertensor from alice to bob\n",
    "jon._objects #will have a pointertensor from jon to bob\n",
    "\n",
    "# we cannot have an operation involving these 2 tensors as their dependency chains r different\n",
    "#z = x + y #returns a TensorNotCollocated Exception\n",
    "    \n",
    "#then how do we get this data???\n",
    "x = x.get() #returns a pointer to the data from the pointer to alice's machine\n",
    "x #pointertensor from me to bob\n",
    "\n",
    "alice._objects #{}. she no longer has the pointer cuz she sent it to us\n",
    "\n",
    "x = x.get()\n",
    "x #output is the tensor [1,2,3,4,5]\n",
    "\n",
    "bob._objects #{}, cuz he sent the tensor to us\n",
    "\n",
    "\n",
    "#GARBAGE COLLECTION\n",
    "\n",
    "bob.clear_objects()\n",
    "alice.clear_objects()\n",
    "\n",
    "x=th.tensor([1,2,3,4,5]).send(bob).send(alice)\n",
    "\n",
    "bob._objects #has the tensor [1,2,3,4,5]\n",
    "alice._objects #pointertensor from alice to bob\n",
    "\n",
    "del x \n",
    "bob._objects #{}\n",
    "alice._objects #{}\n",
    "\n",
    "\"\"\"Garbage collected the whole chain since we deleted the pointer, it shud delete the data that it was pointing to\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#***POINTER CHAIN OPERATIONS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.clear_objects()\n",
    "alice.clear_objects()\n",
    "\n",
    "x=th.tensor([1,2,3,4,5]).send(bob).send(alice)\n",
    "\n",
    "bob._objects #has the tensor [1,2,3,4,5]\n",
    "alice._objects #pointertensor from alice to bob\n",
    "\n",
    "x.remote_get() #using get returns a pointer from alice to bob, but when we use remote_get(),it goes all the way down the pointer\n",
    "#chain and removes data from der...\n",
    "\n",
    "x #will remain a pointer from me to alice\n",
    "bob._objects #{}\n",
    "alice._objects #tensor[1,2,3,4,5]\n",
    "\n",
    "\"\"\"in fact what we try to tell alice is 'Hey, use get() on ur pointer and pull data from bob to urself!' \"\"\"\n",
    "\n",
    "x.move(bob) #move is a convenience operator..it actually takes the pointer and calls remote_get() on it internally\n",
    "x #it will now be a pointer to bob and not alice!\n",
    "\n",
    "bob._objects #tensor[1,2,3,4,5]\n",
    "alice._objects #tensor[1,2,3,4,5]. this is a garbage collection issue as we still have a pointer to it.\n",
    "\n",
    "#Rectified as following example\n",
    "bob.clear_objects()\n",
    "alice.clear_objects()\n",
    "\n",
    "x=th.tensor([1,2,3,4,5]).send(bob)\n",
    "\n",
    "x.move(alice)\n",
    "\n",
    "alice._objects #will have tensor\n",
    "bob._objects #will be empty {}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
