{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and load dataset\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#Defining transform to normalize the data\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,)),])\n",
    "\n",
    "#Donwload and load the training data\n",
    "trainset=datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader=torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) #loads 64 images in each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.pyplot.imshow(images[1].numpy().squeeze(), cmap='Greys_r',);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid activation function\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening the i/p images\n",
    "inputs=images.view(images.shape[0], -1) #shape[0]=64, can use -1 or 784 to flatten out the other dimensions nd retain only batch size\n",
    "\n",
    "#Create parameters\n",
    "w1=torch.randn(784,256)\n",
    "b1=torch.randn(256)\n",
    "\n",
    "w2=torch.randn(256,10)\n",
    "b2=torch.randn(10)\n",
    "\n",
    "h=activation(torch.mm(inputs, w1) + b1)\n",
    "out=torch.mm(h, w2) + b2\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax function to print class probability\n",
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)#torch.sum() takes the sum of all columns in each row\n",
    "probabilities = softmax(out)\n",
    "\"\"\"In the above func, numerator is 64,10 nd denominator torch.sum is just a 64 long vector...this function divides each\n",
    "value in numerator by all 64 values in denom and output will be 64/64 whereas we need 64,10. so we reshape it to have 64 \n",
    "rows and 1 elem in each row. Now each row in num is divided by single value in denom row. hence we get 64,10 tensor\"\"\"\n",
    "\n",
    "#Does it have correct shape as (64,10)?\n",
    "print(probabilities.shape)\n",
    "\n",
    "#Does it sum up to 1?\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the neural network using nn \n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Input to hidden layer linear transformation\n",
    "        self.hidden=nn.Linear(784,256) #Creates weights nd bias by itself\n",
    "        #Output layer, 10 units- one for each digit\n",
    "        self.output=nn.Linear(256,10)\n",
    "        \n",
    "        #define sigmoid activation and softmax output\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Passing i/p tensor through each of the operations\n",
    "        \"\"\"x=self.hidden(x)\n",
    "        x=self.sigmoid(x)\n",
    "        x=self.output(x)\n",
    "        x=self.softmax(x)\"\"\"\n",
    "        #This is cleaner code with functional definiton for sigmoid and softmaz\n",
    "        #Hidden layer with sigmoid activation\n",
    "        x=F.sigmoid(self.hidden(x))\n",
    "        #Output layer with softmax activation\n",
    "        x=F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PROGRAM \"USING BACKPROPAGATION AND CROSS ENTROPY LOSS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "\n",
    "trainset=datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader=torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3035, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Building a feed forward network\n",
    "model=nn.Sequential(nn.Linear(784,128), nn.ReLU(), nn.Linear(128,64),nn.ReLU(), nn.Linear(64,10)) #ReLU=Rectified Linear Unit activation function\n",
    "\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "imag, label = next(iter(trainloader))\n",
    "imag=imag.view(imag.shape[0], -1)\n",
    "\n",
    "#Forward pass to get logits\n",
    "logits=model(imag)\n",
    "\n",
    "#Calculating loss with logits and labels\n",
    "loss=criterion(logits, label)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PROGRAM \"FOR getting log_softmax activation function output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3058, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model=nn.Sequential(nn.Linear(784,128), nn.ReLU(), nn.Linear(128,64),nn.ReLU(), nn.Linear(64,10), nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion=nn.NLLLoss() #Negative log likelihood loss\n",
    "\n",
    "imag, label = next(iter(trainloader))\n",
    "imag=imag.view(imag.shape[0], -1)\n",
    "\n",
    "#Forward pass to get logits\n",
    "logits=model(imag)\n",
    "\n",
    "#Calculating loss with logits and labels\n",
    "loss=criterion(logits, label)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICE PROGRAM FOR GRADIENT AND OPTIMISERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass:\n",
      " None\n",
      "After backward pass:\n",
      " tensor([[-0.0024, -0.0024, -0.0024,  ..., -0.0024, -0.0024, -0.0024],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0036, -0.0036, -0.0036,  ..., -0.0036, -0.0036, -0.0036],\n",
      "        ...,\n",
      "        [-0.0099, -0.0099, -0.0099,  ..., -0.0099, -0.0099, -0.0099],\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [-0.0054, -0.0054, -0.0054,  ..., -0.0054, -0.0054, -0.0054]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before backward pass:\\n\", model[0].weight.grad)#model[0] is for frst linear transformation i.e. Linear(784,128)\n",
    "loss.backward()\n",
    "print(\"After backward pass:\\n\", model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "#Optimizers require parameters of model that has to be optimised nd learning rate as arguments\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) #Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights - \n",
      " Parameter containing:\n",
      "tensor([[-0.0278,  0.0291, -0.0129,  ..., -0.0080,  0.0233,  0.0248],\n",
      "        [ 0.0071, -0.0064,  0.0305,  ...,  0.0344,  0.0008, -0.0018],\n",
      "        [ 0.0220, -0.0235, -0.0259,  ...,  0.0167,  0.0130, -0.0280],\n",
      "        ...,\n",
      "        [-0.0104,  0.0202, -0.0281,  ..., -0.0316, -0.0254, -0.0082],\n",
      "        [ 0.0009,  0.0135, -0.0047,  ...,  0.0033,  0.0011, -0.0312],\n",
      "        [ 0.0273,  0.0218, -0.0266,  ...,  0.0286,  0.0158, -0.0252]],\n",
      "       requires_grad=True)\n",
      "Gradient - \n",
      " tensor([[-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0049,  0.0049,  0.0049,  ...,  0.0049,  0.0049,  0.0049],\n",
      "        ...,\n",
      "        [ 0.0029,  0.0029,  0.0029,  ...,  0.0029,  0.0029,  0.0029],\n",
      "        [-0.0008, -0.0008, -0.0008,  ..., -0.0008, -0.0008, -0.0008],\n",
      "        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial weights - \\n\", model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64,784)\n",
    "\n",
    "#Clearing gradients cuz they might be accumulated\n",
    "optimizer.zero_grad() #very important\n",
    "\n",
    "#Forward pass, then backward pass, then update weights\n",
    "output=model.forward(images)\n",
    "loss=criterion(output, labels)\n",
    "loss.backward()\n",
    "print(\"Gradient - \\n\", model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights - \n",
      " Parameter containing:\n",
      "tensor([[-0.0278,  0.0292, -0.0129,  ..., -0.0080,  0.0233,  0.0249],\n",
      "        [ 0.0071, -0.0064,  0.0305,  ...,  0.0344,  0.0008, -0.0018],\n",
      "        [ 0.0219, -0.0236, -0.0260,  ...,  0.0166,  0.0129, -0.0281],\n",
      "        ...,\n",
      "        [-0.0105,  0.0201, -0.0281,  ..., -0.0317, -0.0254, -0.0082],\n",
      "        [ 0.0009,  0.0135, -0.0047,  ...,  0.0033,  0.0011, -0.0312],\n",
      "        [ 0.0273,  0.0218, -0.0266,  ...,  0.0286,  0.0158, -0.0252]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Weight updation STEP\n",
    "optimizer.step()\n",
    "print(\"Updated weights - \\n\", model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICE PROGRAMS ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL PROGRAM FOR TRAINING PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.8303418678007146 \n",
      "Training loss: 0.7772150211242724 \n",
      "Training loss: 0.5052098759265342 \n",
      "Training loss: 0.4237534903259928 \n",
      "Training loss: 0.3837642736240491 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "\n",
    "trainset=datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader=torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "model=nn.Sequential(nn.Linear(784,128), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(128,64),\n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(64,10), \n",
    "                    nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion=nn.NLLLoss() #Negative log likelihood loss\n",
    "optimizer=optim.SGD(model.parameters(), lr=0.003) #Optimizer\n",
    "\n",
    "epochs=5 #each pass through the entire training set is called epochs\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images=images.view(images.shape[0], -1) #Flattening MNIST images into a 784 long vector\n",
    "        \n",
    "        #TODO: training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output=model.forward(images)\n",
    "        loss=criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img=images[63].view(1,784)\n",
    "#Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits=model.forward(img)\n",
    "\n",
    "#Output of the network are logits. we need to take softmax for probabilities\n",
    "ps=F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1,28,28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
